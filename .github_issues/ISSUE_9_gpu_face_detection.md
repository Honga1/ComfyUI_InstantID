# [Performance] Enable GPU-Accelerated Face Detection with InsightFace

## ğŸ¯ Priority: HIGH
## â±ï¸ Estimated Impact: 50-100% faster face detection (doubles speed)
## ğŸ”§ Difficulty: Hard (requires ONNX Runtime GPU setup)
## âš ï¸ Platform: CUDA, ROCm, or alternative solutions

---

## Problem Description

Face detection with InsightFace is currently **CPU-bound**, creating a major bottleneck:

**Location:** `InstantID.py:238` (InstantIDFaceAnalysis.load_insight_face)

```python
def load_insight_face(self, provider):
    model = FaceAnalysis(
        name="antelopev2",
        root=INSIGHTFACE_DIR,
        providers=[provider + 'ExecutionProvider',]
    )
    model.prepare(ctx_id=0, det_size=(640, 640))
```

### Current Execution Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image (GPU) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ tensor_to_image() - GPUâ†’CPU transfer
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NumPy (CPU) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InsightFace on CPU   â”‚  â—„â”€â”€â”€ BOTTLENECK (100-300ms per image)
â”‚ - Face detection     â”‚
â”‚ - Face recognition   â”‚
â”‚ - Keypoint extractionâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings   â”‚
â”‚ (CPUâ†’GPU)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Impact

**Per image:**
- GPUâ†’CPU transfer: 5-10ms
- Face detection on CPU: **100-300ms** âš ï¸
- CPUâ†’GPU transfer: 5-10ms
- **Total: 110-320ms per image**

**For batch of 6 images (typical workflow):**
- Sequential processing: 6 Ã— 200ms = **1,200ms (1.2 seconds)**
- GPU sits idle during this entire time!

**Workflow impact:**
- Face detection often takes **30-50% of total workflow time**
- GPU utilization drops to 0% during face detection
- Creates stuttering in progress bars (CPU-bound phase)

## Root Cause Analysis

### Why CPU-Only?

InsightFace uses ONNX Runtime with these execution providers:

1. **CPUExecutionProvider** (default) âœ… Always available
2. **CUDAExecutionProvider** âŒ Requires:
   - ONNX Runtime GPU build
   - CUDA Toolkit
   - cuDNN libraries
3. **ROCMExecutionProvider** âŒ Requires:
   - ONNX Runtime ROCm build
   - ROCm stack

**Current code:** Line 238 accepts provider parameter but uses string concatenation:
```python
providers=[provider + 'ExecutionProvider',]
```

**Problem:** Even if user selects "CUDA", it may fall back to CPU silently if ONNX Runtime GPU isn't installed.

### GPUâ†’CPUâ†’GPU Roundtrip Cost

```python
# utils.py:16-19
def tensor_to_image(tensor):
    image = tensor.mul(255).clamp(0, 255).byte().cpu()  # Force CPU
    image = image[..., [2, 1, 0]].numpy()               # Convert to NumPy
    return image
```

This forces CPU transfer before InsightFace even runs. If InsightFace could accept GPU tensors, we'd eliminate both transfers.

## Proposed Solutions

### Option 1: Enable ONNX Runtime GPU Support (Recommended)

**Goal:** Use existing InsightFace with GPU acceleration

**Requirements:**
```bash
# Install ONNX Runtime GPU
pip install onnxruntime-gpu  # For NVIDIA CUDA
# OR
pip install onnxruntime-rocm  # For AMD ROCm
```

**Implementation:**

```python
def load_insight_face(self, provider):
    global _faceanalysis_cache

    cache_key = f"antelopev2_{provider}"
    if cache_key in _faceanalysis_cache:
        return (_faceanalysis_cache[cache_key],)

    # Verify GPU provider is available
    available_providers = onnxruntime.get_available_providers()
    requested_provider = provider + 'ExecutionProvider'

    if requested_provider not in available_providers:
        print(f"\033[33mWARNING: {requested_provider} not available!")
        print(f"Available providers: {available_providers}")
        print(f"Falling back to CPUExecutionProvider\033[0m")
        providers = ['CPUExecutionProvider']
    else:
        providers = [requested_provider]
        print(f"\033[32mINFO: Using {requested_provider} for face detection\033[0m")

    model = FaceAnalysis(
        name="antelopev2",
        root=INSIGHTFACE_DIR,
        providers=providers
    )
    model.prepare(ctx_id=0, det_size=(640, 640))

    _faceanalysis_cache[cache_key] = model
    return (model,)
```

**Benefits:**
- âœ… Drop-in improvement (no major code changes)
- âœ… 50-100% faster face detection
- âœ… Eliminates GPU idle time
- âœ… Better GPU utilization

**Challenges:**
- âŒ Requires users to install onnxruntime-gpu
- âŒ CUDA/cuDNN dependencies
- âŒ May have version compatibility issues

**Expected Impact:**
- Face detection: 100-300ms â†’ **50-150ms** (2Ã— faster)
- Workflow speedup: **10-20%** (if face detection is 30% of time)

### Option 2: PyTorch-Native Face Detection (Long-term)

**Goal:** Replace InsightFace with pure PyTorch implementation

**Options:**
1. **FaceNet PyTorch** - https://github.com/timesler/facenet-pytorch
2. **RetinaFace PyTorch** - https://github.com/biubug6/Pytorch_Retinaface
3. **YOLO Face** - YOLOv8-face for detection
4. **Custom implementation** - Port InsightFace models to PyTorch

**Example with FaceNet:**

```python
from facenet_pytorch import MTCNN, InceptionResnetV1

class PyTorchFaceAnalysis:
    def __init__(self, device='cuda'):
        self.device = device
        # Face detection
        self.detector = MTCNN(
            device=self.device,
            post_process=False
        )
        # Face recognition
        self.recognizer = InceptionResnetV1(
            pretrained='vggface2'
        ).eval().to(self.device)

    def get(self, image_tensor):
        """
        Process image tensor directly on GPU.
        No CPU transfer needed!
        """
        with torch.no_grad():
            # Detect faces
            boxes, probs, landmarks = self.detector.detect(
                image_tensor, landmarks=True
            )

            if boxes is None:
                return []

            # Get embeddings
            faces = self.detector.extract(image_tensor, boxes)
            embeddings = self.recognizer(faces)

            # Format results compatible with InsightFace
            results = []
            for box, landmark, embedding in zip(boxes, landmarks, embeddings):
                results.append({
                    'bbox': box,
                    'kps': landmark,
                    'embedding': embedding.cpu().numpy(),
                    'det_score': probs[0]
                })

            return results
```

**Benefits:**
- âœ… Native GPU execution (no ONNX Runtime)
- âœ… No CPU transfers needed
- âœ… Better integration with PyTorch ecosystem
- âœ… Potentially faster (optimized CUDA kernels)
- âœ… Easier debugging and customization

**Challenges:**
- âŒ Major refactoring required
- âŒ Need to match InsightFace quality
- âŒ May require different models (compatibility)
- âŒ 2-3 weeks implementation time

**Expected Impact:**
- Face detection: 100-300ms â†’ **30-80ms** (3-4Ã— faster)
- Eliminates ALL CPU transfers
- Workflow speedup: **15-25%**

### Option 3: TensorRT Optimization (Advanced)

**Goal:** Convert ONNX models to TensorRT for maximum speed

**Implementation:**

```python
import tensorrt as trt

def convert_to_tensorrt(onnx_model_path):
    """
    Convert InsightFace ONNX models to TensorRT for maximum performance.
    """
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    parser = trt.OnnxParser(network, logger)

    with open(onnx_model_path, 'rb') as model:
        parser.parse(model.read())

    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB

    # Optimize for FP16 if supported
    if builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)

    engine = builder.build_engine(network, config)
    return engine

# Use TensorRT engine for inference
```

**Benefits:**
- âœ… Fastest possible execution on NVIDIA GPUs
- âœ… 2-3Ã— faster than ONNX Runtime GPU
- âœ… Lower latency

**Challenges:**
- âŒ NVIDIA-only (no AMD support)
- âŒ Complex setup and conversion
- âŒ Model conversion can fail
- âŒ Maintenance burden

**Expected Impact:**
- Face detection: 100-300ms â†’ **20-50ms** (4-6Ã— faster)
- Only works on NVIDIA GPUs

## Implementation Plan

### Phase 1: Quick Win - Enable GPU Provider (1 week)
1. Update `load_insight_face` with provider verification
2. Add installation instructions for onnxruntime-gpu
3. Test with CUDA and ROCm
4. Document setup requirements
5. Measure performance improvement

**Deliverables:**
- âœ… GPU-enabled face detection
- âœ… Installation guide
- âœ… Fallback to CPU if GPU unavailable

### Phase 2: Optimize Data Flow (1 week)
1. Investigate keeping tensors on GPU longer
2. Minimize CPU transfers in tensor_to_image
3. Batch processing improvements
4. Parallel face detection for multiple images

**Deliverables:**
- âœ… Reduced transfer overhead
- âœ… Better GPU utilization

### Phase 3: Long-term Solution (Optional, 3-4 weeks)
1. Research PyTorch face detection alternatives
2. Prototype replacement
3. Quality comparison with InsightFace
4. Performance benchmarking
5. Migration guide

**Deliverables:**
- âœ… Pure PyTorch face detection
- âœ… Elimination of ONNX dependency
- âœ… Maximum performance

## Installation & Setup Guide

### For CUDA Users (NVIDIA GPUs)

```bash
# Uninstall CPU-only version
pip uninstall onnxruntime

# Install GPU version
pip install onnxruntime-gpu

# Verify CUDA support
python -c "import onnxruntime as ort; print(ort.get_available_providers())"
# Should include: 'CUDAExecutionProvider'
```

**Requirements:**
- CUDA Toolkit 11.x or 12.x
- cuDNN 8.x
- Compatible GPU (Compute Capability 3.5+)

### For ROCm Users (AMD GPUs)

```bash
# Install ROCm version
pip install onnxruntime-rocm

# Verify ROCm support
python -c "import onnxruntime as ort; print(ort.get_available_providers())"
# Should include: 'ROCMExecutionProvider'
```

**Requirements:**
- ROCm 5.x or 6.x
- Compatible AMD GPU

### Compatibility Matrix

| Platform | Provider | Speed | Setup Difficulty |
|----------|----------|-------|------------------|
| CPU (any) | CPUExecutionProvider | Baseline (1Ã—) | âœ… Easy (default) |
| NVIDIA GPU | CUDAExecutionProvider | 2-3Ã— faster | âš ï¸ Medium (CUDA setup) |
| AMD GPU | ROCMExecutionProvider | 2-3Ã— faster | âš ï¸ Medium (ROCm setup) |
| NVIDIA GPU | TensorRT | 4-6Ã— faster | âŒ Hard (expert) |

## Testing Strategy

### Performance Benchmarks

```python
import time

def benchmark_face_detection(image_batch, provider):
    """
    Benchmark face detection performance.
    """
    model = load_insight_face(provider)

    # Warmup
    for _ in range(3):
        extractFeatures(model, image_batch)

    # Measure
    times = []
    for _ in range(10):
        start = time.perf_counter()
        results = extractFeatures(model, image_batch)
        end = time.perf_counter()
        times.append(end - start)

    avg_time = sum(times) / len(times)
    std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5

    print(f"{provider}: {avg_time*1000:.1f}ms Â± {std_time*1000:.1f}ms")
    return avg_time

# Run benchmarks
for provider in ['CPU', 'CUDA', 'ROCM']:
    benchmark_face_detection(test_images, provider)
```

### Quality Validation

1. **Embedding Consistency**
   - Compare embeddings from CPU vs GPU
   - Cosine similarity should be > 0.999
   - Small numerical differences acceptable

2. **Detection Consistency**
   - Same faces detected in same order
   - Bounding boxes within Â±2 pixels
   - Keypoints within Â±1 pixel

3. **Visual Inspection**
   - Generate outputs with CPU and GPU
   - Verify identical quality
   - No visual artifacts

## Risks and Mitigation

### Risk 1: ONNX Runtime GPU Installation Issues
**Impact:** Users can't install onnxruntime-gpu due to CUDA mismatches

**Mitigation:**
- Provide detailed installation guide
- Automatic fallback to CPU
- Check provider availability at runtime
- Document known issues and workarounds

### Risk 2: Numerical Differences
**Impact:** GPU results slightly different from CPU (floating point)

**Mitigation:**
- Use FP32 precision (not FP16) for face detection
- Verify differences are within tolerance
- Document expected behavior

### Risk 3: Platform Fragmentation
**Impact:** Works on NVIDIA but not AMD, or vice versa

**Mitigation:**
- Test on multiple platforms
- Maintain CPU fallback
- Clear documentation of supported platforms

### Risk 4: Memory Usage
**Impact:** GPU face detection uses more VRAM

**Mitigation:**
- Profile VRAM usage
- Offload models when not in use
- Batch size limits for low-VRAM GPUs

## Success Metrics

### Performance Targets
- âœ… 2Ã— faster face detection on GPU
- âœ… <5% CPU utilization during face detection
- âœ… GPU utilization > 80% throughout workflow

### Quality Targets
- âœ… Embedding similarity > 0.999 vs CPU
- âœ… Detection accuracy maintained
- âœ… Zero regression in visual quality

### Adoption Targets
- âœ… 80%+ GPU users enable GPU detection
- âœ… Installation success rate > 90%
- âœ… <10% support issues related to setup

## References

- **Performance Report:** `PERFORMANCE_REPORT.md` - Issue #9
- **Code Location:** `InstantID.py:238`, `utils.py:16-19`
- **InsightFace:** https://github.com/deepinsight/insightface
- **ONNX Runtime:** https://onnxruntime.ai/docs/execution-providers/
- **FaceNet PyTorch:** https://github.com/timesler/facenet-pytorch

## Related Issues

- Depends on: None (standalone)
- Blocks: None
- Related to: Memory optimization, GPU utilization, workflow speed

---

## Labels
`performance`, `gpu`, `optimization`, `hard`, `high-priority`, `infrastructure`

## Assignees
TBD

## Milestone
v2.0 Performance Improvements
